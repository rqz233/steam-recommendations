[1mdiff --git a/.gitignore b/.gitignore[m
[1mindex 9963d0d..4c4861d 100644[m
[1m--- a/.gitignore[m
[1m+++ b/.gitignore[m
[36m@@ -4,4 +4,12 @@[m [mvenv/[m
 subset*[m
 data/train_users.csv[m
 data/test_users.csv[m
[31m-.vscode/[m
\ No newline at end of file[m
[32m+[m[32m.vscode/[m
[32m+[m[32mdata/*[m
[32m+[m[32mdata/train_users.p[m
[32m+[m[32mdata/test_users.p[m
[32m+[m[32mdata/train_games.p[m
[32m+[m[32mdata/test_games.p[m
[32m+[m[32mboost/bar.csv[m
[32m+[m[32mboost/brand.csv[m
[32m+[m[32mboost/curr_svd_data.csv[m
\ No newline at end of file[m
[1mdiff --git a/boost/predict.py b/boost/predict.py[m
[1mnew file mode 100644[m
[1mindex 0000000..b35e6dd[m
[1m--- /dev/null[m
[1m+++ b/boost/predict.py[m
[36m@@ -0,0 +1,86 @@[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mimport pickle[m
[32m+[m[32mimport math[m
[32m+[m[32mimport scipy.sparse as sparse[m
[32m+[m[32mimport scipy.stats as stats[m
[32m+[m[32mimport matplotlib.pyplot as plt[m
[32m+[m[32mimport scipy.optimize as optimize[m
[32m+[m[32mimport sys[m
[32m+[m[32mfrom surprise.model_selection import cross_validate[m
[32m+[m[32mfrom collections import defaultdict[m
[32m+[m[32mimport pickle[m
[32m+[m[32mimport os[m
[32m+[m[32mimport random[m
[32m+[m[32mfrom surprise import dump[m
[32m+[m[32mfrom surprise import SVD, Reader, Dataset, KNNBasic, KNNWithMeans, \[m
[32m+[m[32m    NormalPredictor, NMF, accuracy[m
[32m+[m
[32m+[m[32m# Parameters[m
[32m+[m[32mdirectory_path = './../'[m
[32m+[m[32mnum = 25  # number of models[m
[32m+[m[32mtarget = '04'  # target sparsity[m
[32m+[m
[32m+[m[32mfile_path = directory_path + 'data/curr_svd_data.csv'[m
[32m+[m[32mfolder_path = directory_path + 'data/boost_' + str(num) + '_' + target + '/'[m
[32m+[m[32mbase_model_path = folder_path + 'boost_model'[m
[32m+[m[32m# path to training dictionary[m
[32m+[m[32mtrain_path = directory_path + 'data/train_users_' + target_name + '.p'[m
[32m+[m[32mtest_path = directory_path + 'data/test_users_' + target_name + '.p'[m
[32m+[m[32mconfidence_path = directory_path + 'data/' + folder_path + 'confidence.p'[m
[32m+[m
[32m+[m[32mtest = test{}[m
[32m+[m[32mwith open(train_path, 'rb') as f:[m
[32m+[m[32m    test = pickle.load(f)[m
[32m+[m[32mf.close()[m
[32m+[m
[32m+[m[32mconfidence = None[m
[32m+[m[32mwith open(confidence_path, 'rb') as f:[m
[32m+[m[32m    confidence = pickle.load(f)[m
[32m+[m
[32m+[m[32mconf_sum = 0[m
[32m+[m[32mfor i in confidence:[m
[32m+[m[32m    conf_sum -= math.log(1/i)[m
[32m+[m[32m    print(i)[m
[32m+[m
[32m+[m
[32m+[m[32mdef write_file(subset, file_name):[m
[32m+[m[32m    with open(file_name, 'w') as f:[m
[32m+[m[32m        for user, game, hours in subset:[m
[32m+[m[32m            f.write('%s\t%s\t%d\n' % (user, game, hours))[m
[32m+[m[32m    f.close()[m
[32m+[m
[32m+[m
[32m+[m[32mreader = Reader(line_format='user item rating',[m
[32m+[m[32m                sep='\t', rating_scale=(0, 6))[m
[32m+[m[32mtest_list = [][m
[32m+[m[32mtrue = [][m
[32m+[m[32mfor user, game_dict in test.items():[m
[32m+[m[32m    for game, hours in game_dict():[m
[32m+[m[32m        true.append(float(hours))[m
[32m+[m[32m        test_list.append((user, game, float(hours)))[m
[32m+[m
[32m+[m[32mtrue = np.array(true)[m
[32m+[m[32mtest_data = Dataset.load_from_file(file_path, reader)[m
[32m+[m
[32m+[m[32mpredictions = np.zeros(len(true), num)[m
[32m+[m
[32m+[m[32mfor i in range(num):[m
[32m+[m[32m    curr_model_path = base_model_path + str(i) + '_' + target + '.p'[m
[32m+[m[32m    _, curr_model = dump.load(curr_model_path)[m
[32m+[m[32m    predicts = curr_model.test(test_data.build_full_trainset)[m
[32m+[m[32m    model_prediction_list = [][m
[32m+[m[32m    for _, _, _, predict, _ in range(predicts):[m
[32m+[m[32m        model_prediction_list.append(predict)[m
[32m+[m
[32m+[m[32msort_inds = np.argsort(predictions, axis=1)[m
[32m+[m
[32m+[m[32mconfidence_mat = confidence[sort_inds][m
[32m+[m[32mconfidence_mat = np.cumsum(confidence_mat, axis=1)[m
[32m+[m[32mweighted_median = np.argmax(confidence_mat > conf_sum/2, axis=1)[m
[32m+[m[32mcorresponding_model = sorted_inds[list([m
[32m+[m[32m    zip(list(range(sorted_inds.shape[0])), weighted_median))][m
[32m+[m[32mfinal_predictions = predictions[zip([m
[32m+[m[32m    list(range(predictions.shape[0])), corresponding_model)][m
[32m+[m
[32m+[m[32merror = np.sqrt(np.mean(np.square(predictions - true)))[m
[32m+[m[32mprint("RMSE: ")[m
[1mdiff --git a/boost/sanity.py b/boost/sanity.py[m
[1mnew file mode 100644[m
[1mindex 0000000..e9a9686[m
[1m--- /dev/null[m
[1m+++ b/boost/sanity.py[m
[36m@@ -0,0 +1,53 @@[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mimport pickle[m
[32m+[m[32mimport math[m
[32m+[m[32mimport scipy.sparse as sparse[m
[32m+[m[32mimport scipy.stats as stats[m
[32m+[m[32mimport matplotlib.pyplot as plt[m
[32m+[m[32mimport scipy.optimize as optimize[m
[32m+[m[32mimport sys[m
[32m+[m[32mfrom surprise.model_selection import cross_validate[m
[32m+[m[32mfrom collections import defaultdict[m
[32m+[m[32mimport pickle[m
[32m+[m[32mimport os[m
[32m+[m[32mimport random[m
[32m+[m[32mfrom surprise import dump[m
[32m+[m[32mfrom surprise import SVD, Reader, Dataset, KNNBasic, KNNWithMeans, \[m
[32m+[m[32m    NormalPredictor, NMF, accuracy[m
[32m+[m
[32m+[m[32mfactors = 100  # number of factors[m
[32m+[m[32muser_reg = .02[m
[32m+[m[32mgame_reg = .02[m
[32m+[m[32muser_vec_reg = .02[m
[32m+[m[32mgame_vec_reg = .02[m
[32m+[m
[32m+[m[32mdirectory_path = 'C:/Users/bpiv4/Dropbox/CIS520/cis520/'[m
[32m+[m[32mfile_path = directory_path + 'data/curr_svd_data.csv'[m
[32m+[m[32mbase_model_path = directory_path + 'data/boost_model'[m
[32m+[m[32m# path to training dictionary[m
[32m+[m[32mtrain_path = directory_path + 'data/train_users_' + target_name + '.p'[m
[32m+[m[32mtest_path = directory_path + 'data/test_users_' + target_name + '.p'[m
[32m+[m
[32m+[m[32mtrain_users = {}[m
[32m+[m[32mwith open(train_path, 'rb') as f:[m
[32m+[m[32m    train_users = pickle.load(f)[m
[32m+[m[32m    f.close()[m
[32m+[m
[32m+[m[32mtest_users = {}[m
[32m+[m[32mwith open(test_path, 'rb') as f:[m
[32m+[m[32m    test_users = pickle.load(f)[m
[32m+[m[32m    f.close()[m
[32m+[m
[32m+[m[32m# associate users with weights[m
[32m+[m[32mtrain_list = [][m
[32m+[m[32mind = 0[m
[32m+[m[32mfor user, game_dict in train_users.items():[m
[32m+[m[32m    for game, hours in game_dict.items():[m
[32m+[m[32m        train_list.append((user, game))[m
[32m+[m[32m        if hours > 6:[m
[32m+[m[32m            print("FUCK")[m
[32m+[m[32m        elif hours < 0:[m
[32m+[m[32m            print("DICK")[m
[32m+[m[32m        game_dict[game] = (float(hours), ind)[m
[32m+[m[32m        ind = ind + 1[m
[32m+[m[32mtrain_list = np.array(train_list, dtype=('str', 'str'))[m
[1mdiff --git a/boost/sanity_comp.py b/boost/sanity_comp.py[m
[1mnew file mode 100644[m
[1mindex 0000000..e86a473[m
[1m--- /dev/null[m
[1m+++ b/boost/sanity_comp.py[m
[36m@@ -0,0 +1,84 @@[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mimport pickle[m
[32m+[m[32mimport math[m
[32m+[m[32mimport scipy.sparse as sparse[m
[32m+[m[32mimport scipy.stats as stats[m
[32m+[m[32mimport matplotlib.pyplot as plt[m
[32m+[m[32mimport scipy.optimize as optimize[m
[32m+[m[32mimport sys[m
[32m+[m[32mfrom surprise.model_selection import cross_validate[m
[32m+[m[32mfrom collections import defaultdict[m
[32m+[m[32mimport pickle[m
[32m+[m[32mimport os[m
[32m+[m[32mimport random[m
[32m+[m[32mfrom surprise import dump[m
[32m+[m[32mfrom surprise import SVD, Reader, Dataset, KNNBasic, KNNWithMeans, \[m
[32m+[m[32m    NormalPredictor, NMF, accuracy[m
[32m+[m[32mimport re[m
[32m+[m
[32m+[m[32mdirectory = "C:/Users/bpiv4/Dropbox/CIS520/cis520/"[m
[32m+[m[32mbran_csv_path = directory + 'boost/bran.csv'[m
[32m+[m[32mbar_csv_path = directory + 'boost/bar.csv'[m
[32m+[m
[32m+[m[32mbar_users = {}[m
[32m+[m[32mbran_users = {}[m
[32m+[m[32mbran_total = 0[m
[32m+[m[32mbar_total = 0[m
[32m+[m[32mwith open(bran_csv_path, 'rb') as f:[m
[32m+[m[32m    for line in f:[m
[32m+[m[32m        line = line.decode('utf-8')[m
[32m+[m[32m        line = line.strip()[m
[32m+[m[32m        line = re.split('\t+', line)[m
[32m+[m[32m        user = line[0].strip()[m
[32m+[m[32m        game = line[1].strip()[m
[32m+[m[32m        hours = line[2].strip()[m
[32m+[m[32m        bran_total = bran_total + 1[m
[32m+[m[32m        if user not in bran_users:[m
[32m+[m[32m            game_dict = {}[m
[32m+[m[32m            game_dict[game] = hours[m
[32m+[m[32m            bran_users[user] = game_dict[m
[32m+[m[32m        else:[m
[32m+[m[32m            game_dict = bran_users[user][m
[32m+[m[32m            game_dict[game] = hours[m
[32m+[m[32m            bran_users[user] = game_dict[m
[32m+[m[32m    f.close()[m
[32m+[m
[32m+[m[32mwith open(bar_csv_path, 'rb') as f:[m
[32m+[m[32m    for line in f:[m
[32m+[m[32m        line = line.decode('utf-8')[m
[32m+[m[32m        line = line.strip()[m
[32m+[m[32m        line = re.split('\t+', line)[m
[32m+[m[32m        user = line[0].strip()[m
[32m+[m[32m        game = line[1].strip()[m
[32m+[m[32m        hours = line[2].strip()[m
[32m+[m[32m        bar_total = bar_total + 1[m
[32m+[m[32m        if user not in bar_users:[m
[32m+[m[32m            game_dict = {}[m
[32m+[m[32m            game_dict[game] = hours[m
[32m+[m[32m            bar_users[user] = game_dict[m
[32m+[m[32m        else:[m
[32m+[m[32m            game_dict = bar_users[user][m
[32m+[m[32m            game_dict[game] = hours[m
[32m+[m[32m            bar_users[user] = game_dict[m
[32m+[m[32m    f.close()[m
[32m+[m
[32m+[m[32mprint('Bran Total: ' + str(bran_total))[m
[32m+[m[32mprint('Bar Total: ' + str(bar_total))[m
[32m+[m
[32m+[m[32mfor user, game_dict in bar_users.items():[m
[32m+[m[32m    for game, hours in game_dict.items():[m
[32m+[m[32m        game_dict[game] = float(hours)[m
[32m+[m
[32m+[m[32mfor user, game_dict in bran_users.items():[m
[32m+[m[32m    for game, hours in game_dict.items():[m
[32m+[m[32m        game_dict[game] = float(hours)[m
[32m+[m
[32m+[m[32mfor user, game_dict in bar_users.items():[m
[32m+[m[32m    bran_dict = bran_users[user][m
[32m+[m[32m    for game, hours in game_dict.items():[m
[32m+[m[32m        bran_hours = bran_dict[game][m
[32m+[m[32m        if hours != bran_hours:[m
[32m+[m[32m            print(user)[m
[32m+[m[32m            print(game)[m
[32m+[m[32m            print('Bar: ' + str(hours))[m
[32m+[m[32m            print('Bran: ' + str(bran_hours))[m
[1mdiff --git a/boost/train.py b/boost/train.py[m
[1mnew file mode 100644[m
[1mindex 0000000..939e7f8[m
[1m--- /dev/null[m
[1m+++ b/boost/train.py[m
[36m@@ -0,0 +1,131 @@[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mimport pickle[m
[32m+[m[32mimport math[m
[32m+[m[32mimport scipy.sparse as sparse[m
[32m+[m[32mimport scipy.stats as stats[m
[32m+[m[32mimport matplotlib.pyplot as plt[m
[32m+[m[32mimport scipy.optimize as optimize[m
[32m+[m[32mimport sys[m
[32m+[m[32mfrom surprise.model_selection import cross_validate[m
[32m+[m[32mfrom collections import defaultdict[m
[32m+[m[32mimport pickle[m
[32m+[m[32mimport os[m
[32m+[m[32mimport random[m
[32m+[m[32mfrom surprise import dump[m
[32m+[m[32mfrom surprise import SVD, Reader, Dataset, KNNBasic, KNNWithMeans, \[m
[32m+[m[32m    NormalPredictor, NMF, accuracy[m
[32m+[m
[32m+[m[32m# Parameters[m
[32m+[m[32mit = 25    # number of iterat ions[m
[32m+[m[32mepochs = 20  # epochs for each model[m
[32m+[m[32mtarget_name = '04'  # density of the data[m
[32m+[m[32msubset_size = 1  # size of subset for each  as a fraction of total data[m
[32m+[m[32mfactors = 100  # number of factors[m
[32m+[m[32muser_reg = .02[m
[32m+[m[32mgame_reg = .02[m
[32m+[m[32muser_vec_reg = .02[m
[32m+[m[32mgame_vec_reg = .02[m
[32m+[m
[32m+[m[32mdirectory_path = 'C:/Users/bpiv4/Dropbox/CIS520/cis520/'[m
[32m+[m[32mfile_path = directory_path + 'data/curr_svd_data.csv'[m
[32m+[m[32mbase_model_path = directory_path + 'data/boost_model'[m
[32m+[m[32m# path to training dictionary[m
[32m+[m[32mtrain_path = directory_path + 'data/train_users_' + target_name + '.p'[m
[32m+[m[32mtest_path = directory_path + 'data/test_users_' + target_name + '.p'[m
[32m+[m
[32m+[m[32mtrain_users = {}[m
[32m+[m[32mwith open(train_path, 'rb') as f:[m
[32m+[m[32m    train_users = pickle.load(f)[m
[32m+[m[32m    f.close()[m
[32m+[m
[32m+[m[32mtest_users = {}[m
[32m+[m[32mwith open(test_path, 'rb') as f:[m
[32m+[m[32m    test_users = pickle.load(f)[m
[32m+[m[32m    f.close()[m
[32m+[m
[32m+[m[32m# associate users with weights[m
[32m+[m[32mtrain_list = [][m
[32m+[m[32mind = 0[m
[32m+[m[32mfor user, game_dict in train_users.items():[m
[32m+[m[32m    for game, hours in game_dict.items():[m
[32m+[m[32m        train_list.append((user, game))[m
[32m+[m[32m        if hours > 6:[m
[32m+[m[32m            print("FUCK")[m
[32m+[m[32m        elif hours < 1:[m
[32m+[m[32m            print("DICK")[m
[32m+[m[32m        game_dict[game] = (float(hours), ind)[m
[32m+[m[32m        ind = ind + 1[m
[32m+[m[32mtrain_list = np.array(train_list, dtype=('str', 'str'))[m
[32m+[m
[32m+[m[32mweights = np.ones(len(train_list))[m
[32m+[m[32mweight_inds = np.arange(start=0, stop=np.size(weights))[m
[32m+[m[32msubset_size = math.ceil(subset_size * len(weights))[m
[32m+[m[32m# functions[m
[32m+[m
[32m+[m
[32m+[m[32mdef write_file(subset, file_name):[m
[32m+[m[32m    with open(file_name, 'w') as f:[m
[32m+[m[32m        for row in subset:[m
[32m+[m[32m            user = row[0][m
[32m+[m[32m            game = row[1][m
[32m+[m[32m            hours = train_users[user][game][0][m
[32m+[m[32m            # print(hours)[m
[32m+[m[32m            f.write('%s\t%s\t%d\n' % (user, game, hours))[m
[32m+[m[32m    f.close()[m
[32m+[m
[32m+[m
[32m+[m[32mbeta = np.zeros(it)[m
[32m+[m[32mav_loss = 0[m
[32m+[m[32m# NOTE change rating scale[m
[32m+[m[32mreader = Reader(line_format='user item rating',[m
[32m+[m[32m                sep='\t', rating_scale=(1, 6))[m
[32m+[m
[32m+[m[32mprint('Setup complete')[m
[32m+[m[32mfor i in range(it):[m
[32m+[m[32m    print('Preparing to train model ' + str(i) + '...')[m
[32m+[m[32m    model_path = base_model_path + str(i) + '_' + target_name + '.p'[m
[32m+[m[32m    probs = np.divide(weights, np.sum(weights))[m
[32m+[m[32m    subset_inds = np.random.choice([m
[32m+[m[32m        weight_inds, subset_size, replace=True, p=probs)[m
[32m+[m[32m    # print(subset_inds.shape)[m
[32m+[m[32m    subset = train_list[subset_inds][m
[32m+[m[32m    # print(subset.shape)[m
[32m+[m[32m    write_file(subset, file_path)[m
[32m+[m[32m    del subset[m
[32m+[m[32m    del subset_inds[m
[32m+[m
[32m+[m[32m    train_data = Dataset.load_from_file([m
[32m+[m[32m        file_path, reader).build_full_trainset()[m
[32m+[m
[32m+[m[32m    print('Training model ' + str(i) + '...')[m
[32m+[m[32m    algo = SVD(verbose=True, n_factors=factors, n_epochs=epochs,[m
[32m+[m[32m               reg_bu=user_reg, reg_bi=game_reg,[m
[32m+[m[32m               reg_pu=user_vec_reg, reg_qi=game_vec_reg)[m
[32m+[m[32m    algo.fit(train_data)[m
[32m+[m
[32m+[m[32m    print('Done Training Model ' + str(i))[m
[32m+[m[32m    print('Saving model...')[m
[32m+[m[32m    predictions = algo.test(train_data.build_test